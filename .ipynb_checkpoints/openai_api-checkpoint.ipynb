{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b62f9815-42f6-4d58-8312-574314f345de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a1d2d05-1b1b-4ded-9095-dd4559d5c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19448bd9-3773-46c0-8a5f-00364cfc1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba1b004-d9bb-4717-b88b-03b4ee445b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dados.json\", encoding=\"utf-8\") as file:\n",
    "    dados = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cebc0d6d-2ae1-48f5-bb51-51ffa2116dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prompt = \"\"\"\n",
    "Considere os dados abaixo que descrevem tarefas de desenvolvimento\n",
    "de software de um projeto real, incluindo requisitos funcionais \n",
    "(FRs), informações do projeto e, em alguns casos, requisitos não \n",
    "funcionais (NFRs). \n",
    "\n",
    "Seu objetivo é recomendar NFRs para tarefas que ainda não os\n",
    "possuem, com base na similaridade com tarefas anteriores. Utilize \n",
    "como critério os campos: 'Descritivo_da_US', 'Modulo', 'Operacao',\n",
    "'Camada', 'Linguagem', 'Tarefa_mapeada' e 'Tarefa_original'.\n",
    "\n",
    "Para cada tarefa que não possui NFR_Tipo, NFR_Atributo ou \n",
    "NFR_Sentença, forneça uma recomendação nos mesmos campos.\n",
    "\n",
    "Formato esperado da resposta:\n",
    "[\n",
    "  {\n",
    "    \"Tarefa_ID\": \"TAxx\",\n",
    "    \"NFR_Tipo\": \"Reliability\",\n",
    "    \"NFR_Atributo\": \"availability\",\n",
    "    \"NFR_Sentença\": \"The system must notify users when the function is unavailable.\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "\n",
    "A recomendação deve ser feita com base nesse novo projeto:\n",
    "[\n",
    "    {\n",
    "        \"fr\": \"Como administrador, desejo editar permissões de acesso a relatórios financeiros.\",\n",
    "        \"dominio\": \"Office/Business\",\n",
    "        \"plataforma\": \"Web\",\n",
    "        \"arquitetura\": \"Client-server\",\n",
    "        \"modulo\": \"Autenticação\",\n",
    "        \"operacao\": \"Editar_permissão\",\n",
    "        \"tecnologias\": {\"Angular\", \"TypeScript\", \"HTML\", \"CSS\"}\n",
    "    }\n",
    "]\n",
    "Aqui estão os dados:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ab8545c-8078-4646-ab0f-d5308721549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(dados, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74f94e0c-ebdf-4029-ab21-22ecfd516150",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = start_prompt + json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9dfb507f-9342-4982-8044-848fd6153b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'o200k_base'>\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 100000\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02a4d050-6a69-4a51-8a40-eb8f90423646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tokens no prompt original: 342767\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(encoding.encode(final_prompt))\n",
    "print(f\"Total de tokens no prompt original: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6dc3e379-a54a-43b4-aefa-b958d072e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_em_chunks(texto, chunk_size_tokens):\n",
    "    tokens = encoding.encode(texto)\n",
    "    chunks = [tokens[i:i + chunk_size_tokens] for i in range(0, len(tokens), chunk_size_tokens)]\n",
    "    return [encoding.decode(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2539ae1-b863-4055-8d06-6546b4e54e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividindo em partes menores...\n",
      "\n",
      "--- RESPOSTA 1 ---\n",
      "\n",
      "Aqui estão as recomendações de requisitos não funcionais (NFRs) para as tarefas que não possuem NFR_Tipo, NFR_Atributo ou NFR_Sentença:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"Tarefa_ID\": \"TA02\",\n",
      "    \"NFR_Tipo\": \"Security\",\n",
      "    \"NFR_Atributo\": \"access_control\",\n",
      "    \"NFR_Sentença\": \"The system must ensure that only authorized users can validate user permissions.\"\n",
      "  },\n",
      "  {\n",
      "    \"Tarefa_ID\": \"TA03\",\n",
      "    \"NFR_Tipo\": \"Performance\",\n",
      "    \"NFR_Atributo\": \"response_time\",\n",
      "    \"NFR_Sentença\": \"In 99% of all cases, the system must have a mean response time of <= 2s for data retrieval operations.\"\n",
      "  },\n",
      "  {\n",
      "    \"Tarefa_ID\": \"TA04\",\n",
      "    \"NFR_Tipo\": \"Security\",\n",
      "    \"NFR_Atributo\": \"confidentiality\",\n",
      "    \"NFR_Sentença\": \"The system must log all user actions related to the management of user permissions.\"\n",
      "  },\n",
      "  {\n",
      "    \"Tarefa_ID\": \"TA09\",\n",
      "    \"NFR_Tipo\": \"Performance\",\n",
      "    \"NFR_Atributo\": \"capacity\",\n",
      "    \"NFR_Sentença\": \"The system must handle up to 100 concurrent connections for data listing operations without performance degradation.\"\n",
      "  },\n",
      "  {\n",
      "    \"Tarefa_ID\": \"TA02\",\n",
      "    \"NFR_Tipo\": \"Security\",\n",
      "    \"NFR_Atributo\": \"authentication\",\n",
      "    \"NFR_Sentença\": \"The system must implement strong password policies that require a minimum of 8 characters with a mix of letters, numbers, and special characters.\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "As NFRs foram escolhidas considerando as semelhanças nas operações, módulos e outros critérios com tarefas que já possuem NFRs correspondentes.\n",
      "\n",
      "--- RESPOSTA 2 ---\n",
      "\n",
      "Parece que você forneceu uma longa lista de requisitos relacionados a vários projetos. Cada objeto contém informações detalhadas sobre as funcionalidades esperadas, incluindo tarefa, operação, esfoço estimado, entre outros. \n",
      "\n",
      "Para ajudá-lo efetivamente, você poderia esclarecer o que exatamente você gostaria de fazer com essas informações? Está buscando um resumo, análise, categorizar os requisitos, ou algo específico?\n",
      "\n",
      "--- RESPOSTA 3 ---\n",
      "\n",
      "The content you provided contains detailed task data for a project management system, focusing on creating a software product with specific features, including front-end and back-end tasks, user stories, and non-functional requirements (NFRs). \n",
      "\n",
      "It appears to be structured in JSON format and covers different projects, sprints, and tasks filled with information such as the project ID, platform, architecture, domain, user stories, module, estimated effort, actual effort, tags, and responsibilities among other fields.\n",
      "\n",
      "To summarize the key components of this data:\n",
      "\n",
      "1. **Projects**: Each project has a unique ID, title, and details regarding its architecture and domain.\n",
      "\n",
      "2. **User Stories (US)**: Each user story describes a requirement from a specific user's perspective, aimed at guiding development.\n",
      "\n",
      "3. **Task Details**: Each task provides information regarding specific operations, including:\n",
      "   - **Operations**: Such as creation, modification, retrieval, and deletion of data.\n",
      "   - **Tags**: Technological tags relevant to each task (e.g., Angular, Springboot).\n",
      "   - **Effort Estimations**: Estimated hours for completion and actual hours spent.\n",
      "\n",
      "4. **Non-Functional Requirements (NFRs)**: These specify quality attributes, such as security, reliability, performance, and availability, that must be met.\n",
      "\n",
      "5. **Responsibilities**: Each task is assigned to an individual, specifying who is responsible for its completion.\n",
      "\n",
      "### Example Summary of a Project Task\n",
      "- **Project ID**: P08\n",
      "- **Project**: Sistema de Gestao\n",
      "- **User Story ID**: US43\n",
      "- **Task ID**: TA04\n",
      "- **Description**: Allow administrators to send personalized notifications to users.\n",
      "- **Module**: Gerencial\n",
      "- **Operation**: Create service for sending notifications via application.\n",
      "- **Tags**: Angular\n",
      "- **Layer**: Front-end\n",
      "- **Estimated Effort**: 4.0 hours\n",
      "- **Actual Effort**: 4.0 hours\n",
      "- **Responsible**: Sheilla da Silva\n",
      "\n",
      "### Purpose\n",
      "Such structured data helps teams effectively plan, track, and manage software development projects while ensuring accountability and clarity about each member's responsibilities and tasks.\n",
      "\n",
      "If you are looking for a specific analysis, extraction of information, or further processing of this data, please specify your needs!\n",
      "\n",
      "--- RESPOSTA 4 ---\n",
      "\n",
      "Parece que você forneceu uma extensa tabela de dados relacionada a um projeto chamado \"Turmalina NFRec\" e \"WebBN\", incluindo várias User Stories (US), Tarefas (TA), e requisitos não funcionais (NFRs). \n",
      "\n",
      "Se você estiver procurando ajuda sobre como analisar, organizar ou extrair informações desse conjunto de dados, sinta-se à vontade para especificar suas perguntas ou interesses específicos. Algumas ações que você pode considerar incluem:\n",
      "\n",
      "1. **Análise de Requisitos**: Avaliando se os requisitos atendem às suas necessidades e se há lacunas nas histórias de usuários (User Stories).\n",
      "\n",
      "2. **Priorização de Tarefas**: Identificando quais tarefas precisam de maior urgência ou esforço para o desenvolvimento.\n",
      "\n",
      "3. **Elaboração de Relatórios**: Gerando resumos ou relatórios sobre o progresso do projeto e o status das tarefas.\n",
      "\n",
      "4. **Identificação de Padrões**: Reconhecendo padrões nos NFRs e suas aplicações.\n",
      "\n",
      "5. **Revisão de Atribuições de Responsabilidades**: Confirmando se as responsabilidades atribuídas são apropriadas para as tarefas.\n",
      "\n",
      "Sinta-se à vontade para me direcionar para o que você gostaria de saber ou realizar!\n"
     ]
    }
   ],
   "source": [
    "if num_tokens > max_tokens:\n",
    "    print(\"Dividindo em partes menores...\")\n",
    "    partes = dividir_em_chunks(final_prompt, max_tokens)\n",
    "\n",
    "    for i, parte in enumerate(partes):\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Você é um especialista em engenharia de requisitos\"},\n",
    "                {\"role\": \"user\", \"content\": parte}\n",
    "            ]\n",
    "        )\n",
    "        print(f\"\\n--- RESPOSTA {i+1} ---\\n\")\n",
    "        print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c485101c-f3e9-4217-95ec-3a8a58f0f7e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-3.5-turbo in organization org-we4ps7OAtGhUkJmvIzpleUfb on tokens per min (TPM): Limit 200000, Requested 287097. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocê é um especialista em engenharia de requisitos\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: final_prompt}\n\u001b[0;32m      6\u001b[0m     ]\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    912\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    913\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    916\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    917\u001b[0m             {\n\u001b[0;32m    918\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    919\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    920\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    921\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    922\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    923\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    924\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    925\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    926\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    927\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    928\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    929\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    930\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    931\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    932\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    933\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    934\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    935\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    936\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    937\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    938\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    939\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    940\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    941\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    942\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    943\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    944\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    945\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    946\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    947\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    948\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m    949\u001b[0m             },\n\u001b[0;32m    950\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    951\u001b[0m         ),\n\u001b[0;32m    952\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    953\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    954\u001b[0m         ),\n\u001b[0;32m    955\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    956\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    957\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    958\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    920\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    921\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    922\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    923\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    924\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    925\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1009\u001b[0m         input_options,\n\u001b[0;32m   1010\u001b[0m         cast_to,\n\u001b[0;32m   1011\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1012\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1013\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1014\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1058\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1059\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1060\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1061\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1062\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1063\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1009\u001b[0m         input_options,\n\u001b[0;32m   1010\u001b[0m         cast_to,\n\u001b[0;32m   1011\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1012\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1013\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1014\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1058\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1059\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1060\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1061\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1062\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1063\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1023\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1022\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1026\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1027\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1032\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for gpt-3.5-turbo in organization org-we4ps7OAtGhUkJmvIzpleUfb on tokens per min (TPM): Limit 200000, Requested 287097. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"Você é um especialista em engenharia de requisitos e vai receber duas solicitações, e na última deve processar os dados presentes nas duas e fornecer a recomendação de requisto com base nos dados e informar quais requisitos funcionais dois dados foram levados em consideração para a decisão\"},\n",
    "        {\"role\":\"user\", \"content\": final_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
