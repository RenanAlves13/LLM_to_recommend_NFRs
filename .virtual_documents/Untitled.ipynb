import json
from langchain.document_loaders import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import Ollama
from langchain.chains import RetrievalQA

# 1. Carregar JSON e transformar em texto
def carregar_json_para_docs(caminho_arquivo):
    with open(caminho_arquivo, 'r', encoding='utf-8') as f:
        dados = json.load(f)

    documentos = []
    for item in dados:
        texto = f"Título: {item['titulo']}\n"
        texto += "Requisitos Funcionais:\n" + "\n".join(item["requisitos_funcionais"]) + "\n"
        texto += "Requisitos Não Funcionais:\n" + "\n".join(item["requisitos_nao_funcionais"]) + "\n"
        documentos.append(Document(page_content=texto))
    return documentos

docs = carregar_json_para_docs("projetos.json")

# 2. Dividir textos longos
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(docs)

# 3. Embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# 4. Base vetorial
db = Chroma.from_documents(chunks, embeddings)

# 5. LLM local via Ollama
llm = Ollama(model="deepseek")

# 6. RAG
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(),
    chain_type="stuff",
    return_source_documents=True
)

# 7. Consultas
resposta = qa("Quais projetos possuem requisito não funcional de desempenho?")
print(resposta["result"])

