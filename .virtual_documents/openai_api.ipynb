import json 
from openai import OpenAI
from dotenv import load_dotenv
import os
import tiktoken


load_dotenv(override=True)
api_key = os.getenv('OPENAI_API_KEY')


openai = OpenAI()


with open("dados.json", encoding="utf-8") as file:
    dados = json.load(file)


start_prompt = """
Considere os dados abaixo que descrevem tarefas de desenvolvimento
de software de um projeto real, incluindo requisitos funcionais 
(FRs), informações do projeto e, em alguns casos, requisitos não 
funcionais (NFRs). 

Seu objetivo é recomendar NFRs para tarefas que ainda não os
possuem, com base na similaridade com tarefas anteriores. Utilize 
como critério os campos: 'Descritivo_da_US', 'Modulo', 'Operacao',
'Camada', 'Linguagem', 'Tarefa_mapeada' e 'Tarefa_original'.

Para cada tarefa que não possui NFR_Tipo, NFR_Atributo ou 
NFR_Sentença, forneça uma recomendação nos mesmos campos.

Formato esperado da resposta:
[
  {
    "Tarefa_ID": "TAxx",
    "NFR_Tipo": "Reliability",
    "NFR_Atributo": "availability",
    "NFR_Sentença": "The system must notify users when the function is unavailable."
  },
  ...
]

A recomendação deve ser feita com base nesse novo projeto:
[
    {
        "fr": "Como administrador, desejo editar permissões de acesso a relatórios financeiros.",
        "dominio": "Office/Business",
        "plataforma": "Web",
        "arquitetura": "Client-server",
        "modulo": "Autenticação",
        "operacao": "Editar_permissão",
        "tecnologias": {"Angular", "TypeScript", "HTML", "CSS"}
    }
]
Aqui estão os dados:
"""


json_data = json.dumps(dados, indent=2, ensure_ascii=False)


final_prompt = start_prompt + json_data


max_tokens = 100000
encoding = tiktoken.encoding_for_model("gpt-4o-mini")
print(encoding)


num_tokens = len(encoding.encode(final_prompt))
print(f"Total de tokens no prompt original: {num_tokens}")


def dividir_em_chunks(texto, chunk_size_tokens):
    tokens = encoding.encode(texto)
    chunks = [tokens[i:i + chunk_size_tokens] for i in range(0, len(tokens), chunk_size_tokens)]
    return [encoding.decode(chunk) for chunk in chunks]


if num_tokens > max_tokens:
    print("Dividindo em partes menores...")
    partes = dividir_em_chunks(final_prompt, max_tokens)

    for i, parte in enumerate(partes):
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Você é um especialista em engenharia de requisitos"},
                {"role": "user", "content": parte}
            ]
        )
        print(f"\n--- RESPOSTA {i+1} ---\n")
        print(response.choices[0].message.content)


response = openai.chat.completions.create(
    model="gpt-4o-mini", 
    messages=[
        {"role":"system","content":"Você é um especialista em engenharia de requisitos e vai receber duas solicitações, e na última deve processar os dados presentes nas duas e fornecer a recomendação de requisto com base nos dados e informar quais requisitos funcionais dois dados foram levados em consideração para a decisão"},
        {"role":"user", "content": final_prompt}
    ]
)

print(response.choices[0].message.content)
